{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import *\n",
    "import string\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99217"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('ferdousi.txt', 'r', encoding='utf-8') as f:\n",
    "    half_verses = f.read()\n",
    "    half_verses = half_verses.split('\\n')[2:]\n",
    "len(half_verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49608"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to remove the last Mesra\n",
    "half_verses = half_verses[:-1]\n",
    "verses = []\n",
    "for i in range(0, len(half_verses), 2):\n",
    "    verses.append(f'{half_verses[i]} {half_verses[i+1]}')\n",
    "len(verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "replace_dict = {}\n",
    "punctuations = '\\.:!،؛؟»\\]\\)\\}«\\[\\(\\{' + string.punctuation\n",
    "\n",
    "with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    var = f.readline()\n",
    "    while var:\n",
    "        stopwords.append(var.strip())\n",
    "        var = f.readline()\n",
    "\n",
    "with open('replace.txt', 'r', encoding='utf-8') as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        key, value = line.split('-')\n",
    "        key, value = key.strip(), value.strip()\n",
    "        replace_dict[f'{key}'] = f'{value}'\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "stemmer = Stemmer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "def replace_function(string):\n",
    "    if string in replace_dict:\n",
    "      return replace_dict[string]\n",
    "    return string\n",
    "\n",
    "def sent_pre_process(sentence, normalize=True, remove_stopwords=False, stemme=False, lemmatize=True, replace=True, remove_punctuations=True, is_first=True):\n",
    "\n",
    "    # replace some charachters\n",
    "    replace_char = {'هٔ': 'ه',\n",
    "                    'ۀ' : 'ه',\n",
    "                    'ه‌ی' : 'ه'}\n",
    "    \n",
    "    if remove_punctuations:\n",
    "      for char in punctuations:\n",
    "        replace_char[char] = \" \"\n",
    "\n",
    "    for key, value in replace_char.items():\n",
    "        sentence = sentence.replace(key, value)\n",
    "\n",
    "    if normalize:\n",
    "        sentence = normalizer.normalize(sentence)\n",
    "    if stemme:\n",
    "        sentence = stemmer.stemme(sentence)\n",
    "    if lemmatize:\n",
    "        sentence = lemmatizer.lemmatize(sentence)\n",
    "    \n",
    "\n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    if replace:\n",
    "        tokens = [replace_function(token) for token in tokens]\n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    if is_first:\n",
    "        return sent_pre_process(\" \".join(tokens), normalize, remove_stopwords, stemme, lemmatize, replace, remove_punctuations, False)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_verses = [sent_pre_process(verse) for verse in verses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['به', 'نام', 'خداوند', 'جان', 'و', 'خرد', 'کزین', 'برتر', 'اندیشه', 'برنگذرد']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_verses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end, pad, unkown = '<s>', '</s>', '<pad>', '<unk>'\n",
    "# add special tokens\n",
    "verse_tokens = [[start] + verse + [end] for verse in processed_verses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = set([word for verse in verse_tokens for word in verse])\n",
    "all_tokens = [pad, unkown] + list(all_tokens)\n",
    "\n",
    "# converting words to numbers\n",
    "word2idx = {word: i for i, word in enumerate(all_tokens)}\n",
    "idx2word = {i: word for word, i in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "unkown_idx = word2idx[unkown]\n",
    "pad_idx = word2idx[pad]\n",
    "start_idx = word2idx[start]\n",
    "end_idx = word2idx[end]\n",
    "\n",
    "def verse_to_numbers(verse):\n",
    "    return [word2idx.get(word, unkown_idx) for word in verse]\n",
    "\n",
    "def numbers_to_verse(numbers):\n",
    "    verse = []\n",
    "    for number in numbers:\n",
    "        if number == end_idx:\n",
    "            break\n",
    "        elif number == unkown_idx:\n",
    "            verse.append(unkown)\n",
    "        verse.append(idx2word[number])\n",
    "    return verse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert verses to list of numbers\n",
    "tokens_idx = [verse_to_numbers(verse) for verse in verse_tokens]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49608, 22])\n",
      "torch.Size([49608, 27])\n"
     ]
    }
   ],
   "source": [
    "X = tokens_idx\n",
    "X = [torch.tensor(x) for x in X]\n",
    "X = pad_sequence(X, batch_first=True, padding_value=pad_idx)\n",
    "print(X.shape)\n",
    "X = F.pad(X, (0, 5, 0, 0), value=pad_idx)\n",
    "print(X.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, Y = X.clone()[1:], X.clone()[:-1]\n",
    "    train_set = TensorDataset(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = int(len(train_set) * 0.9)\n",
    "val_len = len(train_set) - train_len\n",
    "train_set, val_set = random_split(train_set, [train_len, val_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=128, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VersePredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, model_type='lstm',\n",
    "                        num_layers=1, bidirectional=False, dropout=0, max_T=30, pad_idx=pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.model_type = model_type\n",
    "        if model_type == 'lstm':\n",
    "            self.encoder = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "            self.decoder = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        elif model_type == 'gru':\n",
    "            self.encoder = nn.GRU(embedding_dim, hidden_dim, num_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "            self.decoder = nn.GRU(embedding_dim, hidden_dim, num_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.max_T = max_T\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        print('hi')\n",
    "        x = self.embedding(x)\n",
    "        y = self.embedding(y)\n",
    "        _, x = self.encoder(x)\n",
    "        x = x[0] if self.model_type == 'lstm' else x\n",
    "        x = self.dropout(x)\n",
    "        x, _ = self.decoder(y, (x, torch.zeros_like(x)) if self.model_type == 'lstm' else x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        x = self(x, y)\n",
    "        # drop the first word of y\n",
    "        y = y[:, 1:]\n",
    "        x = x[:, :-1]\n",
    "        return F.cross_entropy(x.reshape(-1, x.shape[-1]), y.reshape(-1))\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, x = self.encoder(x)\n",
    "        x = x[0] if self.model_type == 'lstm' else x\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        input_embd = self.embedding(torch.tensor([start_idx]))\n",
    "        input_embds = input_embd.repeat(x.shape[0], 1, 1)\n",
    "        result = torch.zeros(x.shape[0], self.max_T, dtype=torch.long)\n",
    "        for i in range(self.max_T):\n",
    "            x, _ = self.decoder(input_embds, (x, torch.zeros_like(x)) if self.model_type == 'lstm' else x)\n",
    "            x = self.fc(x)\n",
    "            idx = F.softmax(x).multinomial(1)[0]\n",
    "            x = x[idx]\n",
    "            result[:, i] = x.squeeze()\n",
    "            input_embds = self.embedding(x).repeat(x.shape[0], 1, 1)\n",
    "        return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, epochs, device, name='model.pt'):\n",
    "\n",
    "    loss_history = {'train': [], 'val': []}\n",
    "    min_val_loss = float('inf')\n",
    "\n",
    "    for epoch in tqdm(range(epochs), total=epochs, desc='Epochs'):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * x.shape[0]\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            loss = model.loss(x, y)\n",
    "            val_loss += loss.item() * x.shape[0]\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), name)\n",
    "        \n",
    "        print(f'Epoch: {epoch + 1:02} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}')\n",
    "        loss_history['train'].append(train_loss)\n",
    "        loss_history['val'].append(val_loss)\n",
    "\n",
    "    plt.plot(loss_history['train'], label='train')\n",
    "    plt.plot(loss_history['val'], label='val')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VersePredictor(len(word2idx), embedding_dim=128, hidden_dim=256).to(device)\n",
    "lr = 0.001\n",
    "epochs = 15\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train(model, train_loader, val_loader, optimizer, epochs, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on Train and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_samples(set, n=5):\n",
    "    for i in range(n):\n",
    "        idx = np.random.randint(len(set))\n",
    "        x, y = set[idx]\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        print('Input verse:')\n",
    "        print(numbers_to_verse(x.cpu().numpy()))\n",
    "        print('Next verse prediction:')\n",
    "        for i in range(2):\n",
    "            y_pred = model.predict(x.unsqueeze(0))\n",
    "            print(numbers_to_verse(y_pred.cpu().numpy()[0]))\n",
    "        print('Real next verse:')\n",
    "        print(numbers_to_verse(y.cpu().numpy()))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample predictions in validation set\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_samples(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_samples(train_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67bfac4f4aefe1c16f1836a62d55b6e6baa7aba1ac5ce70e93ee8e90eb4f073a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
